{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import tldextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT - IF THE SHAPE OF THE LOADED DATAFRAME CHANGES (5404 rows), THE MANUAL EDITS WILL CREATE MISTAKES\n"
     ]
    }
   ],
   "source": [
    "print('IMPORTANT - IF THE SHAPE OF THE LOADED DATAFRAME CHANGES (5404 rows), THE MANUAL EDITS WILL CREATE MISTAKES')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Get variables and load dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "project_folder = join('/Users','Toavina','githubdata')\n",
    "hn_df_load_folder = join('5.hn_postings_dl','2.pickles','6.aggregate_dataframe')\n",
    "hn_df_filename = 'hn_posts_agg_df.pkl'\n",
    "\n",
    "cleanup_load_folder = '1.manual_clean_files'\n",
    "ghlink_filename = 'hn_df_emails_manual_changes.xlsx'\n",
    "\n",
    "save_folder = join('2.cleaned_df')\n",
    "save_filename = 'cleaned_df.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HN CV posts dataframe\n"
     ]
    }
   ],
   "source": [
    "print('Loading HN CV posts dataframe')\n",
    "hn_df = pickle.load(open(join(project_folder,hn_df_load_folder,hn_df_filename),'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Amend Github account links that were changed manually in Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Excel file to cleanup Github links from Hacker News dataframe\n"
     ]
    }
   ],
   "source": [
    "print('Loading Excel file to cleanup Github links from Hacker News dataframe')\n",
    "email_clean = pd.read_excel(join(cleanup_load_folder,ghlink_filename), header=0, index_col=0)\n",
    "email_clean.index.name=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Removing columns to prevent conflicts, will then be merged by index\n",
    "email_clean = email_clean[['corrected_gh','manually_changed_gh']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging dataframes\n"
     ]
    }
   ],
   "source": [
    "print('Merging dataframes')\n",
    "hn_df = hn_df.join(email_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing manually changed Github account links\n"
     ]
    }
   ],
   "source": [
    "print('Replacing manually changed Github account links')\n",
    "def apply_manual_gh_change(row):\n",
    "    if row['manually_changed_gh'] == 1:\n",
    "        row['github_account'] = row['corrected_gh']\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hn_df = hn_df.apply(apply_manual_gh_change, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing manual email cleanup columns\n"
     ]
    }
   ],
   "source": [
    "print('Removing manual email cleanup columns')\n",
    "\n",
    "hn_df = hn_df.drop(['corrected_gh','manually_changed_gh'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Programmatically cleaning Github accounts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def change_str(string):\n",
    "    \n",
    "    if string == None:\n",
    "        pass\n",
    "    else:\n",
    "        try:\n",
    "            string = str(string)\n",
    "\n",
    "            # Replace username with https://github.com/username\n",
    "            string = re.sub('^(?!.*(github|http))','https://github.com/', string)\n",
    "            \n",
    "            # Replace github.com with https://github.com\n",
    "            string = re.sub(r'^(github.com)\\/', 'https://github.com/' , string)\n",
    "\n",
    "            # Replace http with https\n",
    "            string = re.sub(r'tp:','tps:', string)\n",
    "\n",
    "            # Replace http://www. with https://\n",
    "            string = re.sub(r'www.','', string)\n",
    "        \n",
    "\n",
    "            # Replace https:github.com with https://github.com\n",
    "            string = re.sub(r'https\\:github.com', 'https://github.com', string)\n",
    "\n",
    "            # Replace @username with https://github.com/username\n",
    "            string = re.sub('@','https://github.com/', string)\n",
    "\n",
    "            # Remove [\n",
    "            string = re.sub(r'\\[\\s+', '' , string)\n",
    "\n",
    "            # Remove ]\n",
    "            string = re.sub(r'\\s+\\]','', string)\n",
    "            \n",
    "            # Remove extranuous non-user links\n",
    "            if 'https://github.com' in string and string.count('/') >= 4:\n",
    "                split_str = string.split('/')\n",
    "                string = 'https://github.com/' + split_str[3]\n",
    "            \n",
    "\n",
    "            # Manual changes\n",
    "            string = re.sub(r'https://github.com/dt1/omark','https://github.com/dt1',string)\n",
    "            string = re.sub('fractalide/fractalide','https://github.com/fractalide',string) \n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Programatically changing github_account column to rectify some mistakes\n"
     ]
    }
   ],
   "source": [
    "print('Programatically changing github_account column to rectify some mistakes')\n",
    "hn_df['github_account'] = hn_df['github_account'].apply(change_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Trying to find Github links to add other users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a dataframe filtering for rows with no discernible Github account \n",
    "hn_nogh_accnt = hn_df[pd.isnull(hn_df['github_account'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_github_in_links(df):\n",
    "    \"\"\"Returns a list of indices where the word github appears in the list\"\"\"\n",
    "    gh_in_links = []\n",
    "    for index, value in hn_nogh_accnt['links'].items():\n",
    "        if 'github' in str(value):\n",
    "            gh_in_links.append(index)\n",
    "    gh_inlinks = sorted(list(set(gh_in_links)))\n",
    "    return gh_in_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gh_in_links = find_github_in_links(hn_nogh_accnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Toavina/anaconda/envs/python3/lib/python3.5/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "hn_nogh_accnt['soup'] = hn_nogh_accnt['links']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def change_to_soup(item):\n",
    "    try:\n",
    "        item = str(item)\n",
    "        item = BeautifulSoup(item, 'lxml')\n",
    "    except:\n",
    "        pass\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Toavina/anaconda/envs/python3/lib/python3.5/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "hn_nogh_accnt['soup'] = hn_nogh_accnt['soup'].apply(change_to_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hn_gh_in_links = hn_nogh_accnt.loc[gh_in_links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_gh_links(soup):\n",
    "    \n",
    "    def github_href(href):\n",
    "        \"\"\"Returns only links in the Soup that have github in them\"\"\"\n",
    "        return href and re.compile('github').search(href)\n",
    "    \n",
    "    new_val = soup.find_all('a',href=github_href)[0]['href']\n",
    "    \n",
    "    return new_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hn_gh_in_links['gh_links'] = hn_gh_in_links['soup'].apply(get_gh_links) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting unused column\n"
     ]
    }
   ],
   "source": [
    "print('Deleting unused column')\n",
    "if 'soup' in hn_gh_in_links.columns:\n",
    "    del hn_gh_in_links['soup']\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def change_github_io_to_com(string):\n",
    "    if 'github.io' in string:\n",
    "        string = 'https://github.com/'+tldextract.extract(string).subdomain\n",
    "    \n",
    "    # Remove multiple / after username\n",
    "    if string.count('/') >= 4:\n",
    "        split_str = string.split('/')\n",
    "        string = 'https://github.com/' + split_str[3]\n",
    "        \n",
    "    # Change gist.github.com to github.com\n",
    "    string = re.sub(r'https://gist.github.com','https://github.com',string)\n",
    "    string = re.sub(r'http://gist.github.com','https://github.com',string)\n",
    "        \n",
    "    # Remove ?tab=repositories\n",
    "    string = re.sub(r'\\?tab=repositories','',string)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amending some names for easier extraction\n"
     ]
    }
   ],
   "source": [
    "print('Amending some names for easier extraction')\n",
    "hn_gh_in_links['gh_links'] = hn_gh_in_links['gh_links'].apply(change_github_io_to_com)\n",
    "hn_gh_in_links['gh_links'] = hn_gh_in_links['gh_links'].apply(change_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joining new GH links with main dataframe\n"
     ]
    }
   ],
   "source": [
    "print('Joining new GH links with main dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hn_gh_in_links = hn_gh_in_links[['gh_links']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hn_df = hn_df.join(hn_gh_in_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding new GH users to github_account column\n"
     ]
    }
   ],
   "source": [
    "print('Adding new GH users to github_account column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_new_GH_accounts(row):\n",
    "    \n",
    "    if pd.isnull(row['github_account']) and row['gh_links'] != np.nan:\n",
    "        row['github_account'] = row['gh_links']\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hn_df = hn_df.apply(add_new_GH_accounts, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing gh_links column as unused from now on\n"
     ]
    }
   ],
   "source": [
    "print('removing gh_links column as unused from now on')\n",
    "if 'gh_links' in hn_df.columns:\n",
    "    del hn_df['gh_links']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Picking up Github in resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating slice with resumes that mention linkedin\n"
     ]
    }
   ],
   "source": [
    "print('Creating slice with resumes that mention linkedin')\n",
    "hn_df_in_resume = hn_df[hn_df['resume'].str.contains('github')==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 71 rows which mention Github in the resume for which there the github_account column is empty\n"
     ]
    }
   ],
   "source": [
    "print('There are ' + str(len(hn_df_in_resume[pd.isnull(hn_df_in_resume['github_account'])])) + \\\n",
    "      ' rows which mention Github in the resume for which there the github_account column is empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resumes_to_process_excel_filename = 'resumes_to_process.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumping those rows' resume column to be manually processed as 1.manual_clean_files/resumes_to_process.xlsx\n"
     ]
    }
   ],
   "source": [
    "print(\"Dumping those rows' resume column to be manually processed as \" + \\\n",
    "      join(cleanup_load_folder,resumes_to_process_excel_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resumes_to_process = pd.DataFrame(hn_df_in_resume[pd.isnull(hn_df_in_resume['github_account'])]['resume'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter(join(cleanup_load_folder,resumes_to_process_excel_filename), engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resumes_to_process.to_excel(writer, sheet_name='Sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cleaned up excel as a dataframe\n"
     ]
    }
   ],
   "source": [
    "print('Loading cleaned up excel as a dataframe')\n",
    "cleaned_up_filename = 'resumes_to_process_v1.xlsx'\n",
    "gh_clean = pd.read_excel(join(cleanup_load_folder,cleaned_up_filename), header=0, index_col=0)\n",
    "gh_clean.index.name=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joining dataframes on index\n"
     ]
    }
   ],
   "source": [
    "print('joining dataframes on index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hn_df = hn_df.join(gh_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating github account column with additional users\n"
     ]
    }
   ],
   "source": [
    "print('Updating github account column with additional users')\n",
    "\n",
    "def update_github_column(row):\n",
    "    \"\"\"Updates the github account column with additional user details\"\"\"\n",
    "    if pd.isnull(row['github_account']):\n",
    "        if row['changed_resume'] != np.nan:\n",
    "            row['github_account'] = row['changed_resume']\n",
    "    else:\n",
    "        pass\n",
    "    return row\n",
    "\n",
    "hn_df = hn_df.apply(update_github_column, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting unused columns\n"
     ]
    }
   ],
   "source": [
    "print('Deleting unused columns')\n",
    "\n",
    "if 'original_resume' in hn_df.columns and 'changed_resume' in hn_df.columns:\n",
    "    hn_df = hn_df.drop(['original_resume','changed_resume'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO - Change join to join on original_resume in case original dataframe changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Seeing if can find Github usernames from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 95 Github mentions without a link that could also be searched with enough time\n"
     ]
    }
   ],
   "source": [
    "other_potential_gh_account_num = len(hn_df[(hn_df['github_mention'] == True) & (pd.isnull(hn_df['github_account']))])\n",
    "print('There are ' +str(other_potential_gh_account_num) +' Github mentions without a link that could also be '\\\n",
    "      +'searched with enough time')\n",
    "\n",
    "# TODO - If have time, look at those users to extract additional - marginal gain given only 95 users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Inferring Github usernames from email address by matching with downloaded GH users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO - Match email address from GH user database with email to see how many additional Github accounts I can get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Inferring Github usernames from username (if relevant, mark those)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO for more data - Find matching usernames from HN and Github and see if can find a match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Bonus - If can identify from LN profile and their name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lots of work, but for those that don't have GH username but post a LinkedIn profile - get their LinkedIn profile,\n",
    "# get their name, and then see if that matches a name on the Github database to identify additional accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Inferring Github usernames from link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating column to extract inferred github username\n"
     ]
    }
   ],
   "source": [
    "print('Creating column to extract inferred github username')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def infer_gh_username(gh_link):\n",
    "    \n",
    "    if gh_link == None:\n",
    "        pass\n",
    "    else:\n",
    "        try:\n",
    "            gh_link = str(gh_link)\n",
    "            \n",
    "            gh_link = re.sub('.+github.com/','',gh_link)\n",
    "            gh_link = re.sub('github.com/','', gh_link)\n",
    "            gh_link = re.sub(r'/','', gh_link)\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return gh_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hn_df['inferred_ghuser'] = hn_df['github_account'].apply(infer_gh_username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 918 unique GH users identified that also have an HN account\n"
     ]
    }
   ],
   "source": [
    "print('There are ' +str(len(hn_df[hn_df['inferred_ghuser'] != 'nan']['inferred_ghuser'].unique())) +\\\n",
    "      ' unique GH users identified that also have an HN account')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Pickling dataframe for next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling dataframe to 2.cleaned_df/cleaned_df.pkl\n"
     ]
    }
   ],
   "source": [
    "print('Pickling dataframe to ' + join(save_folder, save_filename))\n",
    "pickle.dump(hn_df, open(join(save_folder, save_filename),'wb'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
