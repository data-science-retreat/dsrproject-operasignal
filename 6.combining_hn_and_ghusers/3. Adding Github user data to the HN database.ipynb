{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-06T16:20:13.330166",
     "start_time": "2016-12-06T16:19:58.892179"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Get variables and load dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-06T16:20:13.346098",
     "start_time": "2016-12-06T16:20:13.333706"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading variables\n"
     ]
    }
   ],
   "source": [
    "print('Loading variables')\n",
    "project_folder = join('/Users','Toavina','githubdata')\n",
    "\n",
    "hn_df_load_folder = join('6.combining_hn_and_ghusers','2.cleaned_df')\n",
    "hn_df_filename = 'cleaned_df.pkl'\n",
    "\n",
    "ghusers_df_load_folder = join('3.gh_users_filter','1.pickles')\n",
    "ghusers_df_filename = 'user_datas_df.pkl'\n",
    "\n",
    "save_folder = join('3.df_w_ghuser_data')\n",
    "save_filename = 'agg_df.pkl'\n",
    "save_filename_w_gh_only = 'agg_df_gh_only.pkl'\n",
    "\n",
    "gh_users_pickles_folder = join(project_folder,'2.gh_userinfo_dl','2.pickles')\n",
    "new_gh_users_pickle_filename = 'hnusers_to_dl_from_gh.pkl'\n",
    "\n",
    "new_gh_df_path = join(ghusers_df_load_folder,'hn_users_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-06T16:20:13.350123",
     "start_time": "2016-12-06T16:20:13.347846"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO - Merge github user dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Merging dataframes by inferred github username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-06T16:20:37.044979",
     "start_time": "2016-12-06T16:20:13.351816"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HN users and Github users dataframes\n"
     ]
    }
   ],
   "source": [
    "print('Loading HN users and Github users dataframes')\n",
    "\n",
    "hn_df = pickle.load(open(join(project_folder,hn_df_load_folder,hn_df_filename),'rb'))\n",
    "ghusers_df = pickle.load(open(join(project_folder,ghusers_df_load_folder,ghusers_df_filename), 'rb'))\n",
    "\n",
    "#Drop duplicate rows from ghusers_df\n",
    "ghusers_df = ghusers_df.drop_duplicates()\n",
    "\n",
    "# Resetting index of ghusers_df\n",
    "ghusers_df = ghusers_df.reset_index().drop('index', axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-06T16:20:37.136958",
     "start_time": "2016-12-06T16:20:37.048544"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading downloaded GH users\n",
      "Removing users not found\n",
      "There are 884 unique users downloaded that have a Github account\n"
     ]
    }
   ],
   "source": [
    "print('Loading downloaded GH users')\n",
    "hn_users_from_gh = pickle.load(open(join(project_folder,new_gh_df_path),'rb'))\n",
    "hn_users_from_gh = hn_users_from_gh.drop_duplicates()\n",
    "hn_users_from_gh = hn_users_from_gh.reset_index().drop('index', axis=1)\n",
    "\n",
    "print('Removing users not found')\n",
    "hn_users_from_gh = hn_users_from_gh[hn_users_from_gh['message'] !='Not Found']\n",
    "\n",
    "print('There are ' + str(len(hn_users_from_gh)) + ' unique users downloaded that have a Github account')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-06T16:20:47.766499",
     "start_time": "2016-12-06T16:20:37.138644"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging GH user dataframes\n",
      "Dropping duplicates\n"
     ]
    }
   ],
   "source": [
    "print('Merging GH user dataframes')\n",
    "gh_users = pd.concat([ghusers_df,hn_users_from_gh])\n",
    "\n",
    "print('Dropping duplicates')\n",
    "gh_users.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-06T16:20:48.920689",
     "start_time": "2016-12-06T16:20:47.768357"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting and saving relevant indices\n"
     ]
    }
   ],
   "source": [
    "print('Resetting and saving relevant indices')\n",
    "\n",
    "# Resetting the index for ghusers_df\n",
    "gh_users = gh_users.reset_index().drop('index', axis =1)\n",
    "\n",
    "# Renaming HN_username to make sure not confusing\n",
    "hn_df = hn_df.rename(columns = {'user': 'hn_username'})\n",
    "\n",
    "# Save old_index so can be restored if needed\n",
    "hn_df['old_index'] = hn_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-06T16:20:48.931992",
     "start_time": "2016-12-06T16:20:48.922249"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amending inferred_ghuser column to remove nan string\n"
     ]
    }
   ],
   "source": [
    "print('Amending inferred_ghuser column to remove nan string')\n",
    "\n",
    "def amend_nans(item_in_inferred_ghuser):\n",
    "    \"\"\"Replaces nans with real NaNs, otherwise run into issues later\"\"\"\n",
    "    if item_in_inferred_ghuser == 'nan':\n",
    "        item_in_inferred_ghuser = np.nan\n",
    "    return item_in_inferred_ghuser\n",
    "\n",
    "hn_df['inferred_ghuser'] = hn_df['inferred_ghuser'].apply(amend_nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-06T16:20:49.554626",
     "start_time": "2016-12-06T16:20:48.934060"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joining GH users database to HN database\n"
     ]
    }
   ],
   "source": [
    "print('Joining GH users database to HN database')\n",
    "# Save inferred ghuser back to refer to if necessary\n",
    "hn_df['inferred_ghuser_copy'] = hn_df['inferred_ghuser']\n",
    "\n",
    "agg_df = hn_df.merge(gh_users,\n",
    "                     how='left',\n",
    "                     left_on='inferred_ghuser',\n",
    "                     right_on='login',\n",
    "                     suffixes=('_hn','_gh')\n",
    "                    )\n",
    "\n",
    "# Drop weird duplicates created by checking for duplicates in old_index, keep first entry\n",
    "agg_df.drop_duplicates(subset='old_index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-06T16:20:49.565688",
     "start_time": "2016-12-06T16:20:49.556249"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1267 events that match in both tables - does not count for multiple messages\n"
     ]
    }
   ],
   "source": [
    "print('There are ' +str(len((agg_df[(~pd.isnull(agg_df['github_account'])) & (~pd.isnull(agg_df['type']))]))) +\\\n",
    "      ' events that match in both tables - does not count for multiple messages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-06T16:20:49.590107",
     "start_time": "2016-12-06T16:20:49.567529"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping duplicates where text and date is the same (i.e. only one entry per month)\n"
     ]
    }
   ],
   "source": [
    "print('Dropping duplicates where text and date is the same (i.e. only one entry per month)')\n",
    "agg_df.drop_duplicates(subset=['text','days_ago'],keep='first', inplace=True)\n",
    "agg_df.drop_duplicates(subset=['hn_username','date'],keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-02T10:35:16.050418",
     "start_time": "2016-12-02T10:35:15.814226"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving aggregate dataframe to 3.df_w_ghuser_data/agg_df.pkl\n",
      "Saving aggregate dataframe containing only GH users to 3.df_w_ghuser_data/agg_df_gh_only.pkl\n"
     ]
    }
   ],
   "source": [
    "print('Saving aggregate dataframe to '+join(save_folder,save_filename))\n",
    "pickle.dump(agg_df, open(join(save_folder,save_filename),'wb'))\n",
    "\n",
    "print('Saving aggregate dataframe containing only GH users to ' + join(save_folder,\n",
    "                                                                      save_filename_w_gh_only))\n",
    "pickle.dump(agg_df[~pd.isnull(agg_df['url'])], open(join(save_folder,save_filename),'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-02T10:35:16.055294",
     "start_time": "2016-12-02T10:35:16.052352"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process complete\n"
     ]
    }
   ],
   "source": [
    "print('Process complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Merging dataframes by matching emails in both dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-02T10:35:16.063350",
     "start_time": "2016-12-02T10:35:16.056995"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Note - not doing this as so few emails - need to download additional users from github\n",
    "\n",
    "#matching_emails = []\n",
    "\n",
    "#for email in tqdm.tqdm_notebook(ghusers_df['email']):\n",
    "#     for contact in hn_df['contact']:\n",
    "#         if email == contact:\n",
    "#             matching_emails.append(email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix. Creating list of Github users to download - Used in Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-02T10:35:16.082555",
     "start_time": "2016-12-02T10:35:16.066779"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print('Creating a list of Github users to download')\n",
    "# gh_accounts_to_dl = agg_df[~pd.isnull(agg_df['github_account'])]['github_account']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-02T10:35:16.103514",
     "start_time": "2016-12-02T10:35:16.084453"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_username(gh_link):\n",
    "    \n",
    "#     username = str(gh_link)\n",
    "#     username = re.sub('https://github.com/','',username)\n",
    "#     username = re.sub('http://github.com/','',username)\n",
    "#     username = re.sub('http://github/','',username)\n",
    "#     username = re.sub('https://github/','',username)\n",
    "#     username = re.sub('github.com/','',username)\n",
    "#     username = re.sub('\\?tab=repositories','',username)\n",
    "#     username = re.sub('Github:','',username)\n",
    "#     username = re.sub('\\-$','',username)\n",
    "#     username = re.sub(' ','',username)\n",
    "#     username = re.sub('/','',username)\n",
    "    \n",
    "    \n",
    "#     return username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-02T10:35:16.108237",
     "start_time": "2016-12-02T10:35:16.105644"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gh_accounts_to_dl = gh_accounts_to_dl.apply(get_username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-02T10:35:16.113219",
     "start_time": "2016-12-02T10:35:16.110404"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for index, value in gh_accounts_to_dl.items():\n",
    "#     print(index,value,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-02T10:35:16.119446",
     "start_time": "2016-12-02T10:35:16.115276"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print('Create manual changes to some usernames - CAUTION - IF INDEX CHANGED (5404 users) NEED TO CHANGE STEP')\n",
    "# manual_changes = [(5232, 'remyferre'),(4154, 'mrmans0n'),(4077, 'CaioBianchi'),\n",
    "#  (4091, np.nan),(3692,'kaymckelly'),(3672, 'joeltaylor'),(3049, 'faun'),\n",
    "#  (2692, 'traverseda'),(1292, 'zura-kh'),(714, 'siscia'), (625, 'martingallagher')]\n",
    "    \n",
    "# for index, value in manual_changes:\n",
    "#     gh_accounts_to_dl.loc[index] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-02T10:35:16.128122",
     "start_time": "2016-12-02T10:35:16.121656"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print('pickling list to download to ' + join(gh_users_pickles_folder,new_gh_users_pickle_filename))\n",
    "# gh_accounts_to_dl = list(gh_accounts_to_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-02T10:35:16.154616",
     "start_time": "2016-12-02T10:35:16.130378"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for each in gh_accounts_to_dl:\n",
    "#     each = str(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-02T10:35:16.181519",
     "start_time": "2016-12-02T10:35:16.156577"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gh_accounts_to_dl = list(set(gh_accounts_to_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-02T10:35:16.208744",
     "start_time": "2016-12-02T10:35:16.183348"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pickle.dump(gh_accounts_to_dl, open(join(gh_users_pickles_folder,new_gh_users_pickle_filename),'wb'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
