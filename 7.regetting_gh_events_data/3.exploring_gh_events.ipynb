{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-02T13:03:30.762335",
     "start_time": "2016-12-02T13:03:30.758646"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "import re\n",
    "from multiprocessing import Pool\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load variables and input csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-02T12:48:39.417526",
     "start_time": "2016-12-02T12:48:39.414127"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_partitions = 7\n",
    "num_cores = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-02T13:03:14.255080",
     "start_time": "2016-12-02T13:03:14.250817"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_file = join('/Users', 'Toavina', 'githubdata',\n",
    "                  '7.regetting_gh_events_data', '2.big_query_results',\n",
    "                  'hn_users_2dec16_ghevents_hnusers-2dec16.csv.gz')\n",
    "\n",
    "save_folder = join('/Users', 'Toavina', 'githubdata',\n",
    "                   '7.regetting_gh_events_data', '3.ghevents_df')\n",
    "save_filename = join('hnusers_ghevents_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-02T12:48:44.003947",
     "start_time": "2016-12-02T12:48:40.084283"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GH events dataframe from zipped csv\n"
     ]
    }
   ],
   "source": [
    "print('Loading GH events dataframe from zipped csv')\n",
    "ghevents_df = pd.read_csv(input_file, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classify events as own repos and organizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-02T12:48:44.010706",
     "start_time": "2016-12-02T12:48:44.005623"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classify_own_repo(row):\n",
    "    \"\"\"Seeks actor_login in the repo_url - if finds a match, reports the action is on the user's own repo\"\"\"\n",
    "    try:\n",
    "        if row['actor_login'] in row['repo_url']:\n",
    "            row['own_repo'] = 1\n",
    "        else:\n",
    "            row['own_repo'] = 0\n",
    "    except:\n",
    "        row['own_repo'] = 0\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-02T12:48:44.017480",
     "start_time": "2016-12-02T12:48:44.012281"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_org(row):\n",
    "    \"\"\"Seeks an org_login and returns 1 if true, 0 if otherwise\"\"\"\n",
    "    try:\n",
    "        if pd.isnull(row['org_login']) == False:\n",
    "            row['org_event'] = 1\n",
    "        else:\n",
    "            row['org_event'] = 0\n",
    "    except:\n",
    "        row['org_event'] = 0\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-02T12:48:44.568309",
     "start_time": "2016-12-02T12:48:44.557692"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parallelize_dataframe(df, func):\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "\n",
    "def parallel_classify_repo(data):\n",
    "    data = data.apply(classify_own_repo, axis=1)\n",
    "    return data\n",
    "\n",
    "\n",
    "def parallel_classify_org(data):\n",
    "    data = data.apply(classify_org, axis=1)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-02T12:53:01.307739",
     "start_time": "2016-12-02T12:48:44.964062"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying events as own repo or not\n"
     ]
    }
   ],
   "source": [
    "print('Classifying events as own repo or not')\n",
    "ghevents_df = parallelize_dataframe(ghevents_df, parallel_classify_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-02T12:57:42.862968",
     "start_time": "2016-12-02T12:53:01.311905"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying events as org or not\n"
     ]
    }
   ],
   "source": [
    "print('Classifying events as org or not')\n",
    "ghevents_df = parallelize_dataframe(ghevents_df, parallel_classify_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Pickling dataframe for next step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-02T13:04:10.394127",
     "start_time": "2016-12-02T13:04:08.321034"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling dataframe for modifying - saved to /Users/Toavina/githubdata/7.regetting_gh_events_data/3.ghevents_df/hnusers_ghevents_df.pkl\n"
     ]
    }
   ],
   "source": [
    "print('Pickling dataframe for modifying - saved to '+ join(save_folder,save_filename))\n",
    "pickle.dump(ghevents_df, open(join(save_folder,save_filename),'wb'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
