# Toavina Andriamanerasoa's Data Science Project - December 2016 - Opera Signal

# Description

**Synopsis: Using Github activity as a signal that technical workers are looking
for a new job**

In 2011, Daniel Doubrovkine posted an article entitled  ‘Github is Your New
Resume’ , in which he claimed that people looking for a new job should use
Github as a portfolio to showcase their skills to employers.

While many articles both corroborating and disparaging that message have
appeared since, if its claims are true, Github activity (combined with other
datasets, such as LinkedIn updates) could be used as a signal that someone is
looking to switch jobs. This kind of signal could be of tremendous value for
recruiters.

As of April 2016 Github had 14 million users, many of which use it with private
repositories or to contribute to open source projects. However, out of those
users, a subset work on public repositories and a subset of those users
(especially since coding bootcamps have gained popularity) have used Github to
showcase their portfolio and coding skills. By identifying the latter’s Github
activity, recruiters  could infer the times they are looking for jobs and
focus on calling those users, thus saving valuable time.

A key challenge in this project is identifying whether people are looking for
jobs. People do not always directly disclose whether they are looking for a job,
but one resource where job seekers advertise is Hacker News, where people can
post their CVs, Github and career website profiles.

This project combines a number of datasets (Github events, Github user profile,
Hacker News and career profiles) to find relationships between anomalous spikes
in Github activity (e.g. number of created, deleted repos…  beyond normal use
over a certain time period) and proxies that confirm that those users are
looking for a job (i.e. when they post their CVs on Hacker News or successful
job changes as identified on career website profiles).

By using anomaly detection to generate signals of abnormal Github activity
patterns, the project seeks patterns that are typically followed by job seeking
(or for successful job switches, a job change). By using other markers (such as
time in latest job, type of previous job, number of Github repos…) from other
databases alongside the signals generated by anomaly detection as inputs into
machine learning models, the project will also seek to qualify relevant signals.

The project then looks at how the model could be further enhanced with other
datasets (Twitter, StackOverflow…) to enhance signal accuracy and provide even
more value for recruiters.

# Disclaimers

## The repo provided is deliberately incomplete

This project is for illustrative purposes only. As it is, the project cannot
be run independently with the files provided in the folder as:

1. It requires the underlying data, which is several tens of GBs / 4TB if using
the entire Github events repo. However, it is useful as the Jupyter notebooks
can show the steps used to transform the data. Someone who is keen to recreate
the project and motivated to do so can however go through the steps and generate
the data requests from scratch.

2. Some parts of the scripts are not provided (notably PDF parser) for various
reasons and so as to not break terms and conditions of certain datasets which
cannot be freely distributed


## No Environment Provided

The project was created using the **Anaconda distribution (Python 3) and various
other libraries.** It is recommended you use Jupyter notebook to view the files.

Due to time constraints and as this project was not written to be shared widely
initially but as a proof of concept, I am not providing a YAML file or
Dockerfile to recreate the environment.

You may however review each of the Python files with the relevant import
statements to find the necessary dependencies at your leisure should you wish
to recreate the project.


## Other disclaimers

You may follow the Jupyter notebooks that are provided (except for
those not provided for reason 2 above) to see the various steps taken and it
explains how various steps were performed, which you might find useful for your
own projects.

Note that the code is provided as is and is not meant to be production code,
rather it is proof of concept to demonstrate the feasibility of the project.

The code was produced under a tight deadline and may not reflect best practices,
but it got things done for a demo day. Implementing the pipeline in production
and refactoring the code for use by others may require significant rewriting.

## License

The project is distributed under an **MIT License**. See the LICENSE.txt file
for further details.


# Folder Description

The folder is organised by steps, with each step output saved so that we can
recover should any problems arise. Some of the steps may be unnecessary but
reflect the steps taken during the project (over 6 weeks), which required
re-running certain steps after discovery of certain results...

The folders are as follows:

1. **gh_events_analysis**
------------------------

Analyses Github events as downloaded from the Google BigQuery public dataset of
Github events (https://cloud.google.com/bigquery/public-data/github)

2. **gh_userinfo_dl**
---------------------

Analyses the users to be downloaded from Github and includes scripts to download
user data by using the Github API

3. **gh_users_filter**
---------------------

Filters Github users to ensure that only relevant ones are downloaded (e.g. ones
with names so can be found on career websites)

5. **hn_postings_dl**
---------------------

Downloads and parses Hacker News Who Wants to Be Hired postings to identify
users with Github accounts that are looking for jobs and advertising them on
Hacker News

6. **combining_hn_and_ghusers**
-------------------------------

Combines Hacker News and Github data to match users and get more complete
profile information


7. **regetting_gh_events_data**
-------------------------------

Redownloads relevant Github events user data according to different criteria


8. **transposing_gh_events**
-----------------------------

Transposing Github events by month and by user to later transform into time
series.

9. **combining_gh_events_hn_users**
-----------------------------------

Combines all datasets together into time series


12. **charting_and_modelling**
------------------------------

Charting combined datasets to look at patterns and graphically see relationships
between Github activity and job seeking status

Algorithms run on data to identify anomalies and create predictive model (using
scikitlearn primarily) on time series and static data, alongside model
testing and validation

A. **bench_scripts**
---------------------

Not necessary - used to time various scripts as performance was paramount given
volume of data

B. **Presentation**
---------------------

Contains financial model showing benefit of using potential project for a
recruiter and a presentation that describes the project in detail.


# Folders deliberately not provided for reasons disclosed above


4. **profile_dl**
---------------------

Contains scripts that attempt to parse career websites data


10. **charting_results**
---------------------

Rendered obsolete by folder 12 and empty.

11. **getting_ln_data**
---------------------

Contains scripts that convert career website data from PDF to Pandas dataframes
via text and line parsing.
